{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee65ce3b",
   "metadata": {},
   "source": [
    "Defines the VideoClipDataset and ConvLSTM model, trains it on clip-level data with train/val loaders, saves per-epoch checkpoints, selects convlstm_best.pth, and evaluates test accuracy and speed plus confusion matrix / precision / recall / F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "545a3afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT exists: True\n",
      "CLIPS_INDEX_PATH exists: True\n",
      "Total clips: 22037\n",
      "By split: Counter({'train': 16020, 'val': 3903, 'test': 2114})\n",
      "By label: Counter({1: 16075, 0: 5962})\n",
      "Sample clip: {'video_path': '/home/olzhas/programming/traffic-accident-edge/TAD-benchmark/train/accident_1/videox3_10.mp4', 'frame_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 'label': 1, 'split': 'train'}\n",
      "train_clips: 16020\n",
      "val_clips:   3903\n",
      "test_clips:  2114\n",
      "Train label counts: Counter({1: 12071, 0: 3949})\n",
      "Val   label counts: Counter({1: 2959, 0: 944})\n",
      "Test  label counts: Counter({0: 1069, 1: 1045})\n",
      "Using device: cuda\n",
      "GPU name: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "\n",
    "PROJECT_ROOT = \"/home/olzhas/programming/traffic-accident-edge\"\n",
    "DATA_ROOT = os.path.join(PROJECT_ROOT, \"TAD-benchmark\")\n",
    "CLIPS_INDEX_PATH = os.path.join(DATA_ROOT, \"clips_index.json\")\n",
    "\n",
    "print(\"DATA_ROOT exists:\", os.path.exists(DATA_ROOT))\n",
    "print(\"CLIPS_INDEX_PATH exists:\", os.path.exists(CLIPS_INDEX_PATH))\n",
    "\n",
    "with open(CLIPS_INDEX_PATH, \"r\") as f:\n",
    "    clips = json.load(f)\n",
    "\n",
    "print(\"Total clips:\", len(clips))\n",
    "\n",
    "split_counts = Counter(c[\"split\"] for c in clips)\n",
    "label_counts = Counter(c[\"label\"] for c in clips)\n",
    "\n",
    "print(\"By split:\", split_counts)\n",
    "print(\"By label:\", label_counts)\n",
    "print(\"Sample clip:\", clips[0])\n",
    "\n",
    "train_clips = [c for c in clips if c[\"split\"] == \"train\"]\n",
    "val_clips = [c for c in clips if c[\"split\"] == \"val\"]\n",
    "test_clips = [c for c in clips if c[\"split\"] == \"test\"]\n",
    "\n",
    "print(f\"train_clips: {len(train_clips)}\")\n",
    "print(f\"val_clips:   {len(val_clips)}\")\n",
    "print(f\"test_clips:  {len(test_clips)}\")\n",
    "\n",
    "print(\"Train label counts:\", Counter(c[\"label\"] for c in train_clips))\n",
    "print(\"Val   label counts:\", Counter(c[\"label\"] for c in val_clips))\n",
    "print(\"Test  label counts:\", Counter(c[\"label\"] for c in test_clips))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf8a6ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample clip shape: torch.Size([16, 3, 112, 112])\n",
      "Sample label: tensor(1)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "CLIP_LEN = 16\n",
    "IMG_SIZE = 112\n",
    "\n",
    "frame_transform = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "class VideoClipDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that returns (clip_tensor, label) for each entry in clips list.\n",
    "    clip_tensor: shape [T, C, H, W] where T = number of frames (e.g. 16)\n",
    "    label: 0 (normal) or 1 (accident)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clips_list, transform=None):\n",
    "        self.clips = clips_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clip_info = self.clips[idx]\n",
    "        video_path = clip_info[\"video_path\"]\n",
    "        frame_indices = clip_info[\"frame_indices\"]\n",
    "        label = clip_info[\"label\"]\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "\n",
    "        for fi in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, fi)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or frame is None:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            if self.transform is not None:\n",
    "                frame = self.transform(frame)\n",
    "            else:\n",
    "                frame = torch.from_numpy(frame).permute(\n",
    "                    2, 0, 1).float() / 255.0\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            clip_tensor = torch.zeros(CLIP_LEN, 3, IMG_SIZE, IMG_SIZE)\n",
    "        else:\n",
    "            while len(frames) < CLIP_LEN:\n",
    "                frames.append(frames[-1].clone())\n",
    "            clip_tensor = torch.stack(frames[:CLIP_LEN], dim=0)\n",
    "\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return clip_tensor, label_tensor\n",
    "\n",
    "\n",
    "train_dataset = VideoClipDataset(train_clips, transform=frame_transform)\n",
    "val_dataset = VideoClipDataset(val_clips,   transform=frame_transform)\n",
    "test_dataset = VideoClipDataset(test_clips,  transform=frame_transform)\n",
    "\n",
    "\n",
    "sample_clip, sample_label = train_dataset[0]\n",
    "\n",
    "print(\"Sample clip shape:\", sample_clip.shape)\n",
    "print(\"Sample label:\", sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b624eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch clips shape: torch.Size([8, 16, 3, 112, 112])\n",
      "Batch labels shape: torch.Size([8])\n",
      "Batch labels sample: tensor([1, 0, 1, 1, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "batch_clips, batch_labels = next(iter(train_loader))\n",
    "print(\"Batch clips shape:\", batch_clips.shape)   # [B, 16, 3, 112, 112]\n",
    "print(\"Batch labels shape:\", batch_labels.shape)\n",
    "print(\"Batch labels sample:\", batch_labels[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1857c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 1.274114 M\n",
      "Input batch shape: torch.Size([8, 16, 3, 112, 112])\n",
      "Logits shape: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic ConvLSTM cell.\n",
    "    Inputs:\n",
    "      x_t:  [B, C_in, H, W]\n",
    "      h_t:  [B, C_hidden, H, W]\n",
    "      c_t:  [B, C_hidden, H, W]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size=3):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=input_dim + hidden_dim,\n",
    "            out_channels=4 * hidden_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=padding\n",
    "        )\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        combined = torch.cat([x, h_prev], dim=1)\n",
    "        conv_out = self.conv(combined)\n",
    "\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(conv_out, self.hidden_dim, dim=1)\n",
    "\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c = f * c_prev + i * g\n",
    "        h = o * torch.tanh(c)\n",
    "        return h, c\n",
    "\n",
    "\n",
    "class SimpleCNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Small CNN to get spatial features from each frame.\n",
    "    Input:  [B, 3, H, W]\n",
    "    Output: [B, C_feat, H_feat, W_feat]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=3, base_channels=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, base_channels,\n",
    "                               kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(base_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            base_channels, base_channels * 2, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(base_channels * 2)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            base_channels * 2, base_channels * 4, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(base_channels * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvLSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Full model: frame-wise CNN encoder + ConvLSTM over time + classifier.\n",
    "    Input:  [B, T, C, H, W]\n",
    "    Output: logits [B, 2] (accident vs normal)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_channels=3, base_channels=32, hidden_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.encoder = SimpleCNNEncoder(\n",
    "            in_channels=img_channels, base_channels=base_channels)\n",
    "\n",
    "        feat_channels = base_channels * 4\n",
    "        self.convlstm_cell = ConvLSTMCell(\n",
    "            input_dim=feat_channels, hidden_dim=hidden_dim, kernel_size=3)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape\n",
    "\n",
    "        x = x.view(B * T, C, H, W)\n",
    "        feats = self.encoder(x)\n",
    "\n",
    "        _, C_feat, H_feat, W_feat = feats.shape\n",
    "        feats = feats.view(B, T, C_feat, H_feat, W_feat)\n",
    "\n",
    "        h = torch.zeros(B, self.convlstm_cell.hidden_dim,\n",
    "                        H_feat, W_feat, device=feats.device)\n",
    "        c = torch.zeros_like(h)\n",
    "\n",
    "        for t in range(T):\n",
    "            x_t = feats[:, t]\n",
    "            h, c = self.convlstm_cell(x_t, h, c)\n",
    "\n",
    "        h_pooled = F.adaptive_avg_pool2d(\n",
    "            h, (1, 1)).view(B, -1)\n",
    "\n",
    "        logits = self.fc(h_pooled)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = ConvLSTMClassifier(\n",
    "    img_channels=3, base_channels=32, hidden_dim=128, num_classes=2)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model params:\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")\n",
    "\n",
    "\n",
    "batch_clips, batch_labels = next(iter(train_loader))\n",
    "batch_clips = batch_clips.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(batch_clips)\n",
    "\n",
    "print(\"Input batch shape:\", batch_clips.shape)\n",
    "print(\"Logits shape:\", logits.shape)  # [B, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75b18ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label counts: Counter({1: 12071, 0: 3949})\n",
      "Class weights (0=normal, 1=accident): tensor([2.0284, 0.6636], device='cuda:0')\n",
      "Checkpoints will be saved to: /home/olzhas/programming/traffic-accident-edge/checkpoints_convlstm\n",
      "\n",
      "=== Epoch 1/5 ===\n",
      "[train] batch 0/2003 loss=0.6726\n",
      "[train] batch 100/2003 loss=0.5319\n",
      "[train] batch 200/2003 loss=0.4213\n",
      "[train] batch 300/2003 loss=0.4229\n",
      "[train] batch 400/2003 loss=0.4668\n",
      "[train] batch 500/2003 loss=0.2866\n",
      "[train] batch 600/2003 loss=0.5107\n",
      "[train] batch 700/2003 loss=0.0703\n",
      "[train] batch 800/2003 loss=0.2772\n",
      "[train] batch 900/2003 loss=0.4377\n",
      "[train] batch 1000/2003 loss=0.0613\n",
      "[train] batch 1100/2003 loss=0.5804\n",
      "[train] batch 1200/2003 loss=0.1105\n",
      "[train] batch 1300/2003 loss=0.4247\n",
      "[train] batch 1400/2003 loss=0.3598\n",
      "[train] batch 1500/2003 loss=0.3019\n",
      "[train] batch 1600/2003 loss=0.1175\n",
      "[train] batch 1700/2003 loss=0.1818\n",
      "[train] batch 1800/2003 loss=0.0177\n",
      "[train] batch 1900/2003 loss=0.1025\n",
      "[train] batch 2000/2003 loss=0.2492\n",
      "Epoch 01: train_loss=0.3135, train_acc=0.8742, val_loss=0.4499, val_acc=0.8014\n",
      "  -> Saved epoch checkpoint to: /home/olzhas/programming/traffic-accident-edge/checkpoints_convlstm/epoch_01_val_0.801.pth\n",
      "  -> New best model (val_acc improved)\n",
      "\n",
      "=== Epoch 2/5 ===\n",
      "[train] batch 0/2003 loss=0.0300\n",
      "[train] batch 100/2003 loss=0.0312\n",
      "[train] batch 200/2003 loss=0.0163\n",
      "[train] batch 300/2003 loss=0.0249\n",
      "[train] batch 400/2003 loss=0.0297\n",
      "[train] batch 500/2003 loss=0.0905\n",
      "[train] batch 600/2003 loss=0.1163\n",
      "[train] batch 700/2003 loss=0.9519\n",
      "[train] batch 800/2003 loss=0.0158\n",
      "[train] batch 900/2003 loss=0.0189\n",
      "[train] batch 1000/2003 loss=0.1817\n",
      "[train] batch 1100/2003 loss=0.0097\n",
      "[train] batch 1200/2003 loss=0.0020\n",
      "[train] batch 1300/2003 loss=0.0308\n",
      "[train] batch 1400/2003 loss=0.0656\n",
      "[train] batch 1500/2003 loss=0.0055\n",
      "[train] batch 1600/2003 loss=0.0005\n",
      "[train] batch 1700/2003 loss=0.0096\n",
      "[train] batch 1800/2003 loss=0.0140\n",
      "[train] batch 1900/2003 loss=0.0024\n",
      "[train] batch 2000/2003 loss=0.0021\n",
      "Epoch 02: train_loss=0.0771, train_acc=0.9752, val_loss=0.7682, val_acc=0.8253\n",
      "  -> Saved epoch checkpoint to: /home/olzhas/programming/traffic-accident-edge/checkpoints_convlstm/epoch_02_val_0.825.pth\n",
      "  -> New best model (val_acc improved)\n",
      "\n",
      "=== Epoch 3/5 ===\n",
      "[train] batch 0/2003 loss=0.0851\n",
      "[train] batch 100/2003 loss=0.7034\n",
      "[train] batch 200/2003 loss=0.0088\n",
      "[train] batch 300/2003 loss=0.0048\n",
      "[train] batch 400/2003 loss=0.0021\n",
      "[train] batch 500/2003 loss=0.0021\n",
      "[train] batch 600/2003 loss=0.0052\n",
      "[train] batch 700/2003 loss=0.0004\n",
      "[train] batch 800/2003 loss=0.0072\n",
      "[train] batch 900/2003 loss=0.0004\n",
      "[train] batch 1000/2003 loss=0.0717\n",
      "[train] batch 1100/2003 loss=0.0081\n",
      "[train] batch 1200/2003 loss=0.0591\n",
      "[train] batch 1300/2003 loss=0.0053\n",
      "[train] batch 1400/2003 loss=0.0046\n",
      "[train] batch 1500/2003 loss=0.1617\n",
      "[train] batch 1600/2003 loss=0.1672\n",
      "[train] batch 1700/2003 loss=0.0142\n",
      "[train] batch 1800/2003 loss=0.0439\n",
      "[train] batch 1900/2003 loss=0.1174\n",
      "[train] batch 2000/2003 loss=0.0133\n",
      "Epoch 03: train_loss=0.0437, train_acc=0.9891, val_loss=0.8442, val_acc=0.8173\n",
      "  -> Saved epoch checkpoint to: /home/olzhas/programming/traffic-accident-edge/checkpoints_convlstm/epoch_03_val_0.817.pth\n",
      "\n",
      "=== Epoch 4/5 ===\n",
      "[train] batch 0/2003 loss=0.0047\n",
      "[train] batch 100/2003 loss=0.0026\n",
      "[train] batch 200/2003 loss=0.1051\n",
      "[train] batch 300/2003 loss=0.0041\n",
      "[train] batch 400/2003 loss=0.2518\n",
      "[train] batch 500/2003 loss=0.0112\n",
      "[train] batch 600/2003 loss=0.0026\n",
      "[train] batch 700/2003 loss=0.0057\n",
      "[train] batch 800/2003 loss=0.0028\n",
      "[train] batch 900/2003 loss=0.0008\n",
      "[train] batch 1000/2003 loss=0.0054\n",
      "[train] batch 1100/2003 loss=0.0010\n",
      "[train] batch 1200/2003 loss=0.0023\n",
      "[train] batch 1300/2003 loss=0.0006\n",
      "[train] batch 1400/2003 loss=0.0004\n",
      "[train] batch 1500/2003 loss=0.2420\n",
      "[train] batch 1600/2003 loss=0.0941\n",
      "[train] batch 1700/2003 loss=0.0210\n",
      "[train] batch 1800/2003 loss=0.0006\n",
      "[train] batch 1900/2003 loss=0.0006\n",
      "[train] batch 2000/2003 loss=0.0427\n",
      "Epoch 04: train_loss=0.0289, train_acc=0.9915, val_loss=0.7996, val_acc=0.7925\n",
      "  -> Saved epoch checkpoint to: /home/olzhas/programming/traffic-accident-edge/checkpoints_convlstm/epoch_04_val_0.792.pth\n",
      "\n",
      "=== Epoch 5/5 ===\n",
      "[train] batch 0/2003 loss=0.0060\n",
      "[train] batch 100/2003 loss=0.0112\n",
      "[train] batch 200/2003 loss=0.0007\n",
      "[train] batch 300/2003 loss=1.5871\n",
      "[train] batch 400/2003 loss=0.0272\n",
      "[train] batch 500/2003 loss=0.0074\n",
      "[train] batch 600/2003 loss=0.0003\n",
      "[train] batch 700/2003 loss=0.0002\n",
      "[train] batch 800/2003 loss=0.0040\n",
      "[train] batch 900/2003 loss=0.0042\n",
      "[train] batch 1000/2003 loss=0.0065\n",
      "[train] batch 1100/2003 loss=0.0038\n",
      "[train] batch 1200/2003 loss=0.0004\n",
      "[train] batch 1300/2003 loss=0.0083\n",
      "[train] batch 1400/2003 loss=0.0014\n",
      "[train] batch 1500/2003 loss=0.0007\n",
      "[train] batch 1600/2003 loss=0.0053\n",
      "[train] batch 1700/2003 loss=0.0218\n",
      "[train] batch 1800/2003 loss=0.0001\n",
      "[train] batch 1900/2003 loss=0.0002\n",
      "[train] batch 2000/2003 loss=0.0024\n",
      "Epoch 05: train_loss=0.0234, train_acc=0.9936, val_loss=0.8249, val_acc=0.8053\n",
      "  -> Saved epoch checkpoint to: /home/olzhas/programming/traffic-accident-edge/checkpoints_convlstm/epoch_05_val_0.805.pth\n",
      "\n",
      "Best val_acc: 0.8253 at epoch 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "\n",
    "train_counts = Counter(c[\"label\"] for c in train_clips)\n",
    "print(\"Train label counts:\", train_counts)\n",
    "\n",
    "total = train_counts[0] + train_counts[1]\n",
    "w0 = total / (2 * train_counts[0])\n",
    "w1 = total / (2 * train_counts[1])\n",
    "\n",
    "class_weights = torch.tensor([w0, w1], device=device, dtype=torch.float32)\n",
    "print(\"Class weights (0=normal, 1=accident):\", class_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "ckpt_dir = os.path.join(PROJECT_ROOT, \"checkpoints_convlstm\")\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "print(\"Checkpoints will be saved to:\", ckpt_dir)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (clips, labels) in enumerate(loader):\n",
    "        clips = clips.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(clips)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * clips.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += clips.size(0)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"[train] batch {batch_idx}/{len(loader)} \"\n",
    "                  f\"loss={loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for clips, labels in loader:\n",
    "            clips = clips.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(clips)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            total_loss += loss.item() * clips.size(0)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += clips.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{NUM_EPOCHS} ===\")\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}: \"\n",
    "          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "\n",
    "    epoch_state = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"val_acc\": val_acc,\n",
    "    }\n",
    "    epoch_ckpt_path = os.path.join(\n",
    "        ckpt_dir,\n",
    "        f\"epoch_{epoch:02d}_val_{val_acc:.3f}.pth\"\n",
    "    )\n",
    "    torch.save(epoch_state, epoch_ckpt_path)\n",
    "    print(\"  -> Saved epoch checkpoint to:\", epoch_ckpt_path)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_state = epoch_state\n",
    "        print(\"  -> New best model (val_acc improved)\")\n",
    "\n",
    "print(f\"\\nBest val_acc: {best_val_acc:.4f} at epoch {best_state['epoch']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dee1b973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best ConvLSTM model to: /home/olzhas/programming/traffic-accident-edge/convlstm_best.pth\n",
      "Best epoch: 2 with val_acc: 0.8252626184985908\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "best_path = os.path.join(PROJECT_ROOT, \"convlstm_best.pth\")\n",
    "torch.save(best_state, best_path)\n",
    "print(\"Saved best ConvLSTM model to:\", best_path)\n",
    "print(\"Best epoch:\", best_state[\"epoch\"],\n",
    "      \"with val_acc:\", best_state[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7d36d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ConvLSTM model from: /home/olzhas/programming/traffic-accident-edge/convlstm_best.pth\n",
      "Best epoch: 2 val_acc: 0.8252626184985908\n",
      "TEST: loss=1.3024, acc=0.7649\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_test = ConvLSTMClassifier(\n",
    "    img_channels=3,\n",
    "    base_channels=32,\n",
    "    hidden_dim=128,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "best_path = os.path.join(PROJECT_ROOT, \"convlstm_best.pth\")\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "model_test.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "print(\"Loaded ConvLSTM model from:\", best_path)\n",
    "print(\"Best epoch:\", ckpt[\"epoch\"], \"val_acc:\", ckpt[\"val_acc\"])\n",
    "\n",
    "test_loss, test_acc = evaluate(model_test, test_loader, device)\n",
    "print(f\"TEST: loss={test_loss:.4f}, acc={test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2058d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM checkpoint exists: True\n",
      "ConvLSTM checkpoint size: 4.87 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "best_path = os.path.join(PROJECT_ROOT, \"convlstm_best.pth\")\n",
    "\n",
    "print(\"ConvLSTM checkpoint exists:\", os.path.exists(best_path))\n",
    "if os.path.exists(best_path):\n",
    "    size_mb = os.path.getsize(best_path) / (1024 * 1024)\n",
    "    print(f\"ConvLSTM checkpoint size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54196c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded ConvLSTM best epoch: 2 val_acc: 0.8252626184985908\n",
      "\n",
      "ConvLSTM TEST accuracy: 0.7649\n",
      "Total clips: 2114\n",
      "Total time: 1199.34 s\n",
      "Avg time per clip: 567.33 ms\n",
      "Clips per second: 1.76\n",
      "Approx frames per second (16 frames/clip): 28.20\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model_test = ConvLSTMClassifier(\n",
    "    img_channels=3,\n",
    "    base_channels=32,\n",
    "    hidden_dim=128,\n",
    "    num_classes=2\n",
    ").to(device)\n",
    "\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "model_test.load_state_dict(ckpt[\"model\"])\n",
    "model_test.eval()\n",
    "\n",
    "print(\"Loaded ConvLSTM best epoch:\",\n",
    "      ckpt[\"epoch\"], \"val_acc:\", ckpt[\"val_acc\"])\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clips, labels in test_loader:\n",
    "        clips = clips.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model_test(clips)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "end_time = time.time()\n",
    "\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_clip = total_time / max(total, 1)\n",
    "clips_per_sec = 1.0 / avg_time_per_clip if avg_time_per_clip > 0 else 0.0\n",
    "\n",
    "\n",
    "frames_per_sec = clips_per_sec * 16\n",
    "\n",
    "test_acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "print(f\"\\nConvLSTM TEST accuracy: {test_acc:.4f}\")\n",
    "print(f\"Total clips: {total}\")\n",
    "print(f\"Total time: {total_time:.2f} s\")\n",
    "print(f\"Avg time per clip: {avg_time_per_clip*1000:.2f} ms\")\n",
    "print(f\"Clips per second: {clips_per_sec:.2f}\")\n",
    "print(f\"Approx frames per second (16 frames/clip): {frames_per_sec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f312712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (rows=true, cols=pred) [accident, normal]:\n",
      " [[973  72]\n",
      " [425 644]]\n",
      "Accident class: precision=0.696, recall=0.931, f1=0.797\n",
      "Normal   class: precision=0.899, recall=0.602, f1=0.722\n",
      "\n",
      "Test label counts: Counter({0: 1069, 1: 1045})\n",
      "Majority baseline accuracy: 0.5056764427625354\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "model_test.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for clips, labels in test_loader:\n",
    "        clips = clips.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model_test(clips)\n",
    "        preds = logits.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy().tolist())\n",
    "        all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "y_true = np.array(all_labels)\n",
    "y_pred = np.array(all_preds)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "print(\"Confusion matrix (rows=true, cols=pred) [accident, normal]:\\n\", cm)\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, labels=[1, 0], zero_division=0\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Accident class: precision={prec[0]:.3f}, recall={rec[0]:.3f}, f1={f1[0]:.3f}\")\n",
    "print(\n",
    "    f\"Normal   class: precision={prec[1]:.3f}, recall={rec[1]:.3f}, f1={f1[1]:.3f}\")\n",
    "\n",
    "test_counts = Counter(c[\"label\"] for c in test_clips)\n",
    "majority_label = max(test_counts, key=test_counts.get)\n",
    "baseline_acc = test_counts[majority_label] / (test_counts[0] + test_counts[1])\n",
    "\n",
    "print(\"\\nTest label counts:\", test_counts)\n",
    "print(\"Majority baseline accuracy:\", baseline_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
